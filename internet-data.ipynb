{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# How The World Wide Web Works\n",
    "\n",
    "---\n",
    "\n",
    "## HTTP\n",
    "\n",
    "Hyper Text Transfer Protocol. \n",
    "\n",
    "HTTP is a protocol, which is to say a set of rules and formats, that allows one computer to get content from another computer. \n",
    "\n",
    "---\n",
    "\n",
    "## Client and Server\n",
    "\n",
    "HTTP is assymetric. One computer requests the content, while the other computer serves the content. \n",
    "\n",
    "The computer requesting the content is the \"client.\"\n",
    "\n",
    "The computer serving the content is the \"server.\"\n",
    "\n",
    "You can think of HTTP as similar to the transaction that happens in a restaurant. Information needs to flow both ways between client and server, but the two parties have very different roles in the transaction. Thus it is an assymetric operation.\n",
    "\n",
    "---\n",
    "\n",
    "## Visiting a web page\n",
    "\n",
    "When you visit twitter.com: \n",
    "\n",
    "* Your browser (i.e. Firefox) is the client.\n",
    "* The server is some custom-built software running in Twitter's cloud infrastructure somewhere.\n",
    "\n",
    "Note that the client and the server are just software, they don't need to be (but usually are) running on separate physical hardware. \n",
    "\n",
    "---\n",
    "\n",
    "## Request/Response\n",
    "\n",
    "* The format that the client uses to send information to the server is called a \"request\".\n",
    "* The format that the server uses to send information to the client is called a \"response\".\n",
    "\n",
    "When you visit twitter.com, your browser (the client) makes an HTTP request, and Twitter's server returns an HTTP response\n",
    "\n",
    "---\n",
    "\n",
    "## HTTP Methods\n",
    "\n",
    "There are several types of requests the client can make. The type of the request is called the \"method.\" There are several HTTP methods the client can choose from, but for now we will focus on one method: \n",
    "\n",
    "GET\n",
    "\n",
    "GET requests are the most common type of request when browsing the internet. It's a way for your client to say \"give me some content.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Requests \n",
    "\n",
    "GET requests consist of: \n",
    "\n",
    "* URL\n",
    "* Additional Metadata\n",
    "\n",
    "Responses consist of: \n",
    "\n",
    "* Body (content)\n",
    "* Status Code\n",
    "* Additional Metadata\n",
    "\n",
    "---\n",
    "\n",
    "## URLs\n",
    "\n",
    "A URL (Universal Resource Locator) is a type of URI (Universal Resource Identifier).\n",
    "\n",
    "A URL is meant to be a unique identifier for a \"resource\", or a piece of content, that can be accessed via the world wide web. \n",
    "\n",
    "A URL consists of: \n",
    "  \n",
    "\n",
    " protocol     [subdomain.]      domain       [:port]    [/path]    [?query]\n",
    "----------   --------------  ------------   ---------  ---------  -----------\n",
    " http://        blog.         science.com     :443       /foo       ?bar=baz\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Protocol \n",
    "\n",
    "HTTP is unencrypted. \n",
    "\n",
    "HTTPS is encrypted. \n",
    "\n",
    "---\n",
    "\n",
    "## Domains\n",
    "\n",
    "Every computer connected to the internet lives at a certain address (domain/subdomain pair).\n",
    "\n",
    "---\n",
    "\n",
    "## Ports\n",
    "\n",
    "Ports are like doors. \n",
    "\n",
    "Every computer connected to the internet has thousands of potential ports.\n",
    "\n",
    "A server is a piece of software that runs on a computer, and \"listens\" for HTTP requests on a certain port.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Default Ports\n",
    "\n",
    "* HTTP requests default to port 80\n",
    "* HTTPS requests default to port 443\n",
    "\n",
    "Most web pages serve content on the default ports, and as such, we drop the port from the URL.\n",
    "\n",
    "---\n",
    "\n",
    "## Servers\n",
    "\n",
    "It's the internet's job to direct the HTTP request to the right address. \n",
    "\n",
    "It's the computer's job to direct the HTTP request to the right port. \n",
    "\n",
    "Then, it's up to the server listening on that port to send a response. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Path\n",
    "\n",
    "From one server, we often want to serve lots of different content. \n",
    "\n",
    "Like the menu of a restaraunt. \n",
    "\n",
    "A path is a unique identifier for each piece of content the server can provide (each item on the menu)\n",
    "\n",
    "Paths are trees! Just like menus are organized categorically by their creators (burritos, tacos, tostadas), content within the server is organized with a taxonomy that is created by whoever wrote the server software.\n",
    "\n",
    "---\n",
    "\n",
    "## Query String\n",
    "\n",
    "In a restaraunt, the food might be like: \n",
    "\n",
    "- Burritos\n",
    " -- Chimichangas\n",
    " -- Grande\n",
    " -- Children's\n",
    "\n",
    "While each burrito might have the following options:\n",
    "\n",
    "Meat: Asada, Chorizo, Lengua, Soyrizo, Tofu\n",
    "Cheese: Queso Fresco, Vegan Cheese\n",
    "\n",
    "---\n",
    "\n",
    "## Query String\n",
    "\n",
    "Sometimes we want to allow options for each piece of content. A query string is a collection of key-value pairs (like a Python dictionary) that describe the content requested.\n",
    "\n",
    "A Grande Burrito with Soyrzo and Vegan Cheese would look like this in a URL: \n",
    "\n",
    "https://myburrito.com/burritos/grande?meat=soyrizo&cheese=vegan\n",
    "\n",
    "---\n",
    "\n",
    "## Servers\n",
    "\n",
    "Paths and query strings are nothing but a way for the server to organize its content for the client. There are no strict rules, as long as the client can learn the format and the server is happy with the organization.\n",
    "\n",
    "---\n",
    "\n",
    "## Headers\n",
    "\n",
    "All metadata sent in HTTP requests and responses lives in the \"headers\". \n",
    "\n",
    "The headers are just a collection of key-value pairs.\n",
    "\n",
    "Status codes live in the headers. The key is \"status\" and the value is the code number! \n",
    "\n",
    "Another common header in HTTP Responses exists under the key \"Content-Type\". This tells the client what type of content is in the body, and thus how to decode it.\n",
    "\n",
    "---\n",
    "\n",
    "## Content-Types\n",
    "\n",
    "There are 3 main content types you should know about: \n",
    "\n",
    "* Plain text\n",
    "* HTML\n",
    "* JSON\n",
    "\n",
    "---\n",
    "\n",
    "## Plain Text\n",
    "\n",
    "Plain text is the simplest form of content. It is not used very often in actual applications. \n",
    "\n",
    "---\n",
    "\n",
    "## JSON\n",
    "\n",
    "JavaScript Object Notation.\n",
    "\n",
    "An \"object\" in JavaScript is similar to a Dictionary in python. It's an associative data structure, a collection of key-value pairs! \n",
    "\n",
    "JSON is the most common format for sending data over HTTP when the data is meant to be consumed by a computer program, rather than presented for a human to view.\n",
    "\n",
    "API's (application programming interfaces) commonly use JSON to send data.\n",
    "\n",
    "---\n",
    "\n",
    "## JSON\n",
    "\n",
    "An example of JSON: \n",
    "\n",
    "```{js}\n",
    "\n",
    "{\n",
    "    \"id\": \"b4vd345s45gd\",\n",
    "    \"tweets\": [12543, 9878945, 90384],\n",
    "    \"profile\": { \n",
    "        \"name\": \"Man Onthe Moon\",\n",
    "        \"location\": \"moon\"\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## HTML\n",
    "\n",
    "Hyper Text Markup Language. \n",
    "\n",
    "HTML is a format used to encode content, so that it can be displayed for humans to read in a web browser. \n",
    "\n",
    "HTML tells the browser what to display, and how to display it. \n",
    "\n",
    "For example: if you have a heading (title) followed by two paragraphs. You need to tell the browser not only about the order of the text, but to make the heading larger and bolder, and to separate the paragraphs with a new line!\n",
    "\n",
    "---\n",
    "\n",
    "## HTML\n",
    "\n",
    "HTML is a tree. It organizes all the content for the browser into a hierarchical taxonomy. \n",
    "\n",
    "```{html}\n",
    "\n",
    "                  |-- meta qux\n",
    "       |-- head --|\n",
    "       |          |-- script baz\n",
    "html --| \n",
    "       |          |-- div.foo\n",
    "       |-- body --|\n",
    "                  |-- div.bar\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## HTML\n",
    "\n",
    "The root node is called \"html\", which has only two possible child nodes, \"head\" and \"body.\" Those two nodes can have unlimited children. \n",
    "\n",
    "```{html}\n",
    "<html>\n",
    "    <head>\n",
    "        ...\n",
    "    </head>\n",
    "    <body> \n",
    "        <div class=\"foo\"></div>\n",
    "        <div class=\"bar\"></div>\n",
    "    </body>\n",
    "</html>\n",
    "\n",
    "```\n",
    "\n",
    "## HTML or JSON??? \n",
    "\n",
    "JSON and HTML are used in two very different contexts: \n",
    "\n",
    "* HTML is used to create a \"UI\", a user interface, for consumption by human eyes. \n",
    "\n",
    "* JSON is used in an \"API\", an application programming interface, for consumption by other computer programs.\n",
    "\n",
    "It should be clear that, in general, you should prefer APIs for getting data, wherever they are available. \n",
    "\n",
    "Getting data out of HTML, gotten from websites, is referred to as \"scraping\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Making HTTP Requests\n",
    "\n",
    "In Python, there are many libraries to make HTTP requests. We will use a 3rd-party library called \"requests\", which is very easy to use. \n",
    "\n",
    "Making a \"GET\" request is as simple as: \n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "res = requests.get(url) # returns a \"Response\" object\n",
    "res.content # has the \"body\" of the response\n",
    "```\n",
    "\n",
    "You might need to install the requests library! \n",
    "\n",
    "You can do that with the following code in a Jupyter cell: \n",
    "\n",
    "```python\n",
    "! pip install requests\n",
    "```\n",
    "\n",
    "Or, if you're using anaconda, optionally you can also do: \n",
    "\n",
    "```python\n",
    "! conda install -c anaconda requests\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Parsing JSON data\n",
    "\n",
    "To parse JSON data in Python, we will use the \"json\" module: \n",
    "\n",
    "```python\n",
    "import json\n",
    "```\n",
    "\n",
    "Read more about the module on the [documentation page](https://docs.python.org/3/library/json.html)!\n",
    "\n",
    "All we care about for this part is the method \"loads\", which turns JSON data into a Python object: \n",
    "\n",
    "```python\n",
    "json.loads(my_string_encoded_json)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pokemon API\n",
    "\n",
    "There is a simple, open API called \"pokeapi\" that allows us to make requests and see how to use APIs. Like everything, we first look at the documentation: \n",
    "\n",
    "https://pokeapi.co/docs/v2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see how to make a get request to the API: \n",
    "import requests\n",
    "import json\n",
    "\n",
    "res = requests.get('https://pokeapi.co/api/v2/berry')\n",
    "json.loads(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Challenge: \n",
    "# Create a Dataframe with all the Pokemon names and their URLs. \n",
    "\n",
    "def get_pokes(url):\n",
    "    # Make the HTTP request to the given url. \n",
    "    # Parse the response as json\n",
    "    # return the \"next\" and the \"results\" (as a 2-tuple!)\n",
    "    # make sure to return a \"falsey\" value (such as None)\n",
    "    # if there is not a \"next!\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def catch_em_all(url):\n",
    "    pokes = []\n",
    "    \n",
    "    # While loop! Like a for-loop, \n",
    "    # but goes on for an indetermined amount\n",
    "    # of time (while condition is truthy):\n",
    "    while url:\n",
    "        url, results = get_pokes(url)\n",
    "        pokes += results\n",
    "    return pokes\n",
    "        \n",
    "    \n",
    "list_of_pokes = catch_em_all('https://pokeapi.co/api/v2/pokemon')\n",
    "\n",
    "# This data is most naturally represented as a list of dictionaries. \n",
    "# How can we create a dataframe from a list of dictionaries? \n",
    "# Try to find out on your own, from the internet!\n",
    "\n",
    "# TODO: turn list_of_pokes into a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Scraping!\n",
    "\n",
    "---\n",
    "\n",
    "## Terminology: Crawling vs Scraping\n",
    "\n",
    "Web crawling refers to the act of traversing the internet by a bot, of a piece of software.\n",
    "\n",
    "Search engines, traditionally, crawl the internet and don't do much with the content except index the text.\n",
    "\n",
    "Scraping refers to the act of extracting specific data from web pages.\n",
    "\n",
    "Often, scraping involves some crawling, and uses the same libraries and techniques. Thus, the terms and concepts will overlap.\n",
    "\n",
    "---\n",
    "\n",
    "## Scraping\n",
    "\n",
    "Scraping consists of:\n",
    "\n",
    "1. Making HTTP requests to servers for HTML content.\n",
    "2. Parsing that HTML content to:\n",
    "   1. Store desired information.\n",
    "   2. Find links to follow (for each link, go back to 1.)\n",
    "\n",
    "---\n",
    "\n",
    "## Making HTTP Requests\n",
    "\n",
    "Browsers make HTTP requests.\n",
    "\n",
    "Every major programming language also has a way to make HTTP requests.\n",
    "\n",
    "This is sometimes done via a third-party library.\n",
    "\n",
    "---\n",
    "\n",
    "## Parsing HTML\n",
    "\n",
    "HTML parsing is something you will use a trusted library for.\n",
    "\n",
    "HTML parsers take the raw HTML from an HTTP response, and turn it into a (usually custom) tree-like data structure or class that you can easily traverse, and from which you can easily extract the desired content.\n",
    "\n",
    "This functionality is conceptually different from that of making the HTTP request. It will usually be included in a separate library, for this reason.\n",
    "\n",
    "You will need to learn the API, the interface, of the HTML parser you are using!\n",
    "\n",
    "Read the documentation.\n",
    "\n",
    "---\n",
    "\n",
    "## Following Links\n",
    "\n",
    "Making an HTTP request and parsing it is all you need to scrape a single page.\n",
    "\n",
    "Usually, however, we want to scrape more than one page.\n",
    "\n",
    "How do we get all the pages we will scrape?\n",
    "\n",
    "Often, we get them from links in other pages!\n",
    "\n",
    "---\n",
    "\n",
    "## Following Links\n",
    "\n",
    "What are some websites you might want to scrape?\n",
    "\n",
    "Which pages?\n",
    "\n",
    "How can we access all the pages?\n",
    "\n",
    "---\n",
    "\n",
    "## Sitemaps\n",
    "\n",
    "Some sites provide a link to an XML sitemap in their robots.txt file (more on that later).\n",
    "\n",
    "Other sites provide a sitemap directly as an HTML page, labelled \"sitemap\".\n",
    "\n",
    "Still others provide no sitemap at all.\n",
    "\n",
    "Sitemaps are generally meant as a way in which crawlers can easily get to all the pages on the site. There might also be some sort of \"directory\" pages for part of the content.\n",
    "\n",
    "---\n",
    "\n",
    "## Rules of the road\n",
    "\n",
    "What can you scrape?\n",
    "\n",
    "What should you scrape?\n",
    "\n",
    "What is legal to scrape?\n",
    "\n",
    "---\n",
    "\n",
    "## Public vs. Private\n",
    "\n",
    "There are two types of content on the web: that which everybody can see (public), and that which only certain individuals can see (private).\n",
    "\n",
    "When you login to a website, you usually see some private content. You also agreed to a legal document, whether you read it or not, their Terms and Conditions!\n",
    "\n",
    "Those Terms and Conditions can, and often do, make it illegal for you to scrape private content. And because you have agreed to them, you are bound by them.\n",
    "\n",
    "---\n",
    "\n",
    "## Public vs. Private\n",
    "\n",
    "Public content, on the other hand, is less black and white.\n",
    "\n",
    "There are websites who are happy with you scraping their content, as long as you do it politely.\n",
    "\n",
    "There are others that don't want you scraping their content unless they know who you are. Almost everyone wants Google to crawl and index all their pages. But they may not want their competitor doing the same!\n",
    "\n",
    "---\n",
    "\n",
    "## Being Polite\n",
    "\n",
    "Let's assume you have the website's blessing.\n",
    "\n",
    "How does one act politely?\n",
    "\n",
    "1. Follow robots.txt file\n",
    "2. Scrape slowly\n",
    "3. Identify yourself\n",
    "\n",
    "---\n",
    "\n",
    "## Robots.txt\n",
    "\n",
    "Most major websites will have a robots.txt file. This is just a text file that they create in order to tell bots (web crawlers and scrapers) the rules of their website.\n",
    "\n",
    "You should obey robots.txt files, it's part of being a good citizen on the web!\n",
    "\n",
    "Let's look at an example. Canonically, they are always at /robots.text:\n",
    "\n",
    "<https://www.airbnb.com/robots.txt>\n",
    "\n",
    "Mostly, they just describe which paths bots are allowed to access, and which they are not.\n",
    "\n",
    "---\n",
    "\n",
    "## Scraping Speed\n",
    "\n",
    "HTTP requests take time to complete, even at the speed of light, the data might have to go all around the world and back.\n",
    "\n",
    "While an HTTP request is being made, your computer, and it's processor, is idling. Your processor can prepare many other requests, and handle many other responses, while waiting for its first HTTP request to complete (it could be hundreds of milliseconds!).\n",
    "\n",
    "---\n",
    "\n",
    "## Scraping Speed\n",
    "\n",
    "Scraping in parallel can happen, depending on the language, via processes, threads, or an asynchronous event loop.\n",
    "\n",
    "Modern machines can thus make many requests very quickly!\n",
    "\n",
    "However, servers are limited in how many requests they can handle at a given time. For this reason, they prefer to spread the load of requests as evenly as possible, avoiding large spikes in usage.\n",
    "\n",
    "For this reason, they want you to scrape slowly.\n",
    "\n",
    "---\n",
    "\n",
    "## Identify Yourself\n",
    "\n",
    "One Header that you can send with an HTTP request is that of \"user-agent\".\n",
    "\n",
    "In the case of normal web browsing, \"user-agent\" refers to the exact browser and version being used.\n",
    "\n",
    "In the case of scraping, however, it is polite to use a name that refers to your bot and your website, thus that identifies you uniquely so their engineers can know who you are.\n",
    "\n",
    "---\n",
    "\n",
    "## Problems\n",
    "\n",
    "* Getting Blocked\n",
    "* Javascript\n",
    "\n",
    "---\n",
    "\n",
    "## Getting blocked\n",
    "\n",
    "If you scrape too quickly, scrape from a commercial IP address (AWS), or the website doesn't know you, you might be blocked from crawling. Instead of giving you the page you asked for, they might give you a different page, potentially with a 403 status code, that tells you that you have been denied access.\n",
    "\n",
    "There are many ways around being blocked. You can lie about your user-agent, use proxies, hold on to cookies, etc.\n",
    "\n",
    "In general, however, you should be careful. Even if you feel you are ethically justified, this can be a slipper cat and mouse game that eats up a lot of your time!\n",
    "\n",
    "It might be easier just to ask the website to whitelist you!\n",
    "\n",
    "---\n",
    "\n",
    "## Javascript\n",
    "\n",
    "We have been focusing on the paradigm wherein the server responds to HTTP GET requests with HTML.\n",
    "\n",
    "Sometimes, however, not all the HTML that we see in our browser is actually sent by the server. The server might send a small amount of HTML, along with some Javascript code, whose responsibility it is to generate the rest of the HTML.\n",
    "\n",
    "This poses a major problem for scraping, as the content we want isn't returned by the server!\n",
    "\n",
    "---\n",
    "\n",
    "## Javascript\n",
    "\n",
    "The solution is to use a headless browser. \n",
    "\n",
    "A headless browser is just a browser that does not render the content to a UI. \n",
    "\n",
    "Headless browsers can be embedded within your scraping program via a library, or run as a separate piece of software and accessed over HTTP. \n",
    "\n",
    "Popular options: Selenium and Splash\n",
    "\n",
    "---\n",
    "\n",
    "## Storing Data\n",
    "\n",
    "There are many options for storing data from scraping: \n",
    "\n",
    "* Flat files (json lines, csv, etc.)\n",
    "* Database\n",
    "\n",
    "---\n",
    "\n",
    "## Following Many Links\n",
    "\n",
    "Let's return to the problem of following links. \n",
    "\n",
    "Often, the links grow expontentially in number as we scrape. \n",
    "\n",
    "This is because one page in a directory or search results might link to 10-20 \"detail\" pages which are often the ones we actually want data from. \n",
    "\n",
    "In other words, we might want to scrape thousands or millions of individual pages, but we won't have that list of pages ahead of time, we will build it as we go. \n",
    "\n",
    "---\n",
    "\n",
    "## Following Many Links\n",
    "\n",
    "How can we deal with this ever-growing list? \n",
    "\n",
    "* Loops in loops in loops\n",
    "* Recursive function calls\n",
    "* A queue\n",
    "\n",
    "Which of these will work in a distributed or parallel framework? \n",
    "\n",
    "Elegantly, only the quee.\n",
    "\n",
    "---\n",
    "\n",
    "## Production Scraping\n",
    "\n",
    "When you actually want to scrape a large site, you will need to make requests in parallel to get the speed needed. \n",
    "\n",
    "You can build this yourself quite simply, or use a scraping library that gives you this for free. \n",
    "\n",
    "Scrapy is a Python library that gives you this, and much more, for free. It's a highly opinionated and structured library, so there is a learning curve, but it is well documented and popular. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Parsing HTML\n",
    "\n",
    "---\n",
    "\n",
    "## HTML\n",
    "\n",
    "Each node of the tree is an \"HTML element.\"\n",
    "\n",
    "Some common elements:\n",
    "\n",
    "```{html}\n",
    "<div>\n",
    "<p>\n",
    "<span>\n",
    "<h1>\n",
    "<a>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## HTML\n",
    "\n",
    "In addition to having children and/or text, each element can have \"attributes.\" Some common attributes are \"id\" and \"class\":\n",
    "\n",
    "```{html}\n",
    "<div id=\"foo\">\n",
    "    <span class=\"email\"> man@themoon.space </span>\n",
    "</div>\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## HTML\n",
    "\n",
    "Elements, classes, and ids give us a way to traverse the HTML tree and target a specific node (and its subtree!)\n",
    "\n",
    "This is very important. This is used in styling webpages as well as in web scraping.\n",
    "\n",
    "Let's see an example:\n",
    "\n",
    "---\n",
    "\n",
    "## HTML\n",
    "\n",
    "```{html}\n",
    "<body>\n",
    "    <div class=\"foo\">\n",
    "        <h3> EMAIL </h3>\n",
    "        <span id=\"email\"> man@themoon.space </span>\n",
    "    </div>\n",
    "    <article class=\"bar\">\n",
    "        <span> My Day </span>\n",
    "        <p> Hello, I would like to discuss...</p>\n",
    "    </article>\n",
    "</body>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## HTML\n",
    "\n",
    "\n",
    "Using CSS notation, we can target the email via:\n",
    "\n",
    "```{css}\n",
    "div.foo span#email\n",
    "```\n",
    "\n",
    "Additionally, we could simplify it to:\n",
    "\n",
    "```{css}\n",
    ".foo span\n",
    "```\n",
    "\n",
    "Because there is only one element with the class \"foo\", and only one span element inside that!\n",
    "\n",
    "Or, because there is an id, we can use that and nothing else:\n",
    "\n",
    "```{css}\n",
    "#email\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## HTML\n",
    "\n",
    "(example with browser inspector on live webpage)\n",
    "\n",
    "---\n",
    "\n",
    "## HTML\n",
    "\n",
    "Some elements have special attributes.\n",
    "\n",
    "Anchor tags can have an \"href\" attribute, which is a link to another page. Anchor links and hrefs form the basis of the internet!\n",
    "\n",
    "```{html}\n",
    "<a href=\"https://man.mars/redmanred\">\n",
    "    Checkout my boy's homepage!\n",
    "</a>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# We will see how we can use a library called Beautiful Soup to parse\n",
    "# the html. Getting the correct node out of an HTML tree can be tricky, \n",
    "# but there are many tutorials online and the Beautiful Soup documentation is\n",
    "# a great place to start!)\n",
    "\n",
    "# Install \"beautifulsoup4\" if you don't have it! (pip or conda)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_soup(url):\n",
    "    res = requests.get(url)\n",
    "    return BeautifulSoup(res.text)\n",
    "\n",
    "\n",
    "soup = get_soup('https://brickset.com/sets/year-2016')\n",
    "\n",
    "soup.select('.set h1 a')\n",
    "\n",
    "# This gets the titles first page, how can we follow the pages to get ALL the titles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_titles(soup):    \n",
    "    \"\"\" Returns a list of titles on the page \"\"\"\n",
    "    return [s.get_text() for s in soup.select('.set h1 a')]\n",
    "\n",
    "def get_next(soup):\n",
    "    try:\n",
    "        return soup.select('li.next a')[0]['href']\n",
    "    except IndexError:\n",
    "        return None\n",
    "\n",
    "def get_soup(url):\n",
    "    res = requests.get(url)\n",
    "    return BeautifulSoup(res.text)\n",
    "\n",
    "def parse_bricks(url):\n",
    "    \"\"\" Fetches Lego Bricks page and extracts titles \"\"\"\n",
    "    # Use a while loop to get all brickset titles\n",
    "    # gather them into a list\n",
    "    # and return the whole list\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "bricks = parse_bricks('https://brickset.com/sets/year-2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "assert(bricks[0] == '10251:  Brick Bank')\n",
    "assert(bricks[9] == '10722:  Snake Showdown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Project: Live Exchange Rates\n",
    "\n",
    "Imagine that you work with financial assets which are denominated in different currencies. You analyze this data regularly, and want to create a \"transformation\" function that transforms all your assets into EUR prices, based on today's exchange rate. \n",
    "\n",
    "Your data with the local-currency-denominated value of each asset lives in a file called \"assets.csv\" which should be located in the same folder as this notebook. \n",
    "\n",
    "Write a \"data loading\" function that: \n",
    "\n",
    "1. Reads the data, given the path to the file. \n",
    "2. Returns a dataframe with an additional column that has the assets value in euros, as of today.\n",
    "\n",
    "Use this free API to get today's exchange rates: https://exchangeratesapi.io/. You will need to read the documentation and try it out to see how it works. \n",
    "\n",
    "HINT: Write a separate function to get the current exchange rates! That can be reused!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "name": "internet-data.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
